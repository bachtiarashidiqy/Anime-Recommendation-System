# -*- coding: utf-8 -*-
"""Anime-Recommendation-System_Bachtiar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FBM25VZ5XtFJo3-pEMaQeyB0dBgPa8cb

# Anime Recommendation System Using Content-Based Filtering and K-Nearest Neighbor

## Project Description
### Background Description of the Anime Recommendation System Using Content-Based Filtering and K-Nearest Neighbor
This project aims to develop a recommendation system that can suggest top-N anime titles to users based on their preferences using content-based filtering and the K-Nearest Neighbor (KNN) algorithm. Currently, many recommendation systems rely on collaborative filtering, which can suffer from cold-start problems and data sparsity. By implementing content-based filtering, the system can provide personalized recommendations even with limited user interaction data, enhancing user experience and engagement.

## Data Collection

Import Libraries
"""

from google.colab import files
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score

"""Kaggle configuration"""

!pip install -q kaggle

files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle
!chmod 600 ~/.kaggle/kaggle.json

"""[Anime Dataset 2023](https://www.kaggle.com/datasets/dbdmobile/myanimelist-dataset/data?select=anime-filtered.csv)

Download datasets from kaggle
"""

!kaggle datasets download -d dbdmobile/myanimelist-dataset

"""Extract zip"""

!unzip /content/myanimelist-dataset.zip

"""We can see that after we extract the zip file, there are many csv files, but in this case we only use the `anime-filtered.csv` file.

Read anime-filtered.csv
"""

df = pd.read_csv('/content/anime-filtered.csv')

"""## Data Understanding"""

df.head()

print(f'This dataset has {df.shape[0]} rows and {df.shape[1]} columns.')

df.info()

"""Based on the provided output, the following conclusions can be drawn:

There are a total of 25 columns in the dataframe.

* Out of these, 15 columns are of data type object, which are most likely categorical features. These columns are: Name, Genres, English Name, Japanese Name, Synopsis, Type, Episodes, Aired, Premiered, Producers, Licensors, Studios, Source, Duration, and Rating.

* Additionally, there are 2 columns with data type float64, which are likely numerical features. These columns are: Score and Ranked.

* The remaining 8 columns are of data type int64, which may also serve as numerical features. These columns include: anime_id, Popularity, Members, Favorites, Watching, Completed, On-Hold, and Dropped.

Check for duplicate data
"""

df.duplicated().sum()

"""There is no duplicate data

## Exploratory Data Analysis

### Variabel Description
"""

df.columns

"""**Data information**

The dataset's columns contain the following information:

* anime_id: Unique ID for each anime (number or identification code).
* Name: The original title of the anime.
* Score: The rating or score assigned to the anime.
* Genres: The genres of the anime, separated by commas (e.g., Action, Comedy, Fantasy).
* English Name: The English title of the anime (if available).
* Japanese Name: The Japanese title of the anime (if available).
* Synopsis: A brief description or plot summary of the anime.
* Type: The type of anime (e.g., TV Series, Movie, OVA, etc.).
* Episodes: The number of episodes in the anime.
* Aired: The original airing date of the anime.
* Premiered: The season and year of the anime's debut.
* Producers: The production companies or producers of the anime.
* Licensors: The license holders of the anime (e.g., streaming platforms).
* Studios: The animation studios involved.
* Source: The source material of the anime (e.g., manga, light novel, original).
* Duration: Duration of each episode.
* Rating: Age rating for viewers.
* Ranked: The ranking of the anime based on popularity or other criteria.
* Popularity: The popularity rank of the anime.
* Members: The number of members who added the anime to their lists on the platform.
* Favorites: The number of users who marked the anime as favorite.
* Watching: The number of users currently watching the anime.
* Completed: The number of users who have finished watching.
* On Hold: The number of users who paused watching.
* Dropped: The number of users who discontinued watching.

### Visualizations
"""

import matplotlib.pyplot as plt

# Count the number of entries for each type
ona_count = df.loc[df['Type'] == 'ONA'].shape[0]
tv_count = df.loc[df['Type'] == 'TV'].shape[0]
movie_count = df.loc[df['Type'] == 'Movie'].shape[0]
music_count = df.loc[df['Type'] == 'Music'].shape[0]
special_count = df.loc[df['Type'] == 'Special'].shape[0]
ova_count = df.loc[df['Type'] == 'OVA'].shape[0]

# Labels and corresponding colors
labels = ['ONA', 'TV', 'Movie', 'Music', 'Special', 'OVA']
colors = ['#81F4E1', '#56CBF9', '#F5D491', '#BEB7A4', '#B4E1FF', '#F06C9B']

# Plotting
plt.figure(figsize=(10, 7))
plt.title('Anime Categories Distribution', fontsize=16, fontweight='bold', color='#333333', pad=20)

# Create pie chart with enhanced aesthetics
plt.pie(
    [ona_count, tv_count, movie_count, music_count, special_count, ova_count],
    labels=labels,
    colors=colors,
    autopct=lambda p: '{:.2f}%'.format(p) if p > 0 else '',  # Show two decimal places
    startangle=140,
    wedgeprops={'edgecolor': 'white', 'linewidth': 1.5}
)

# Add a legend outside the pie to avoid overlapping with title
plt.legend(
    labels,
    loc='center left',
    bbox_to_anchor=(1, 0.5),
    fontsize=12,
    frameon=False
)

plt.axis('equal')
plt.tight_layout()
plt.show()

# Histogram Rating
plt.figure(figsize=(12, 6))
plt.hist(df['Score'], color='#B4E1FF', edgecolor='black')
plt.ylabel('Total', fontsize=14)
plt.xlabel('Avg Rating', fontsize=14)
plt.title("Anime's Average Ratings Distribution", fontsize=16, fontweight='bold')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

df.describe().T

# Top 10 Anime based on Members
plt.figure(figsize=(20, 15))
top10_members = df[['Name', 'Members']].sort_values(by='Members', ascending=False).head(10)

colors_members = ['#87255B', '#56CBF9', '#F5D491', '#BEB7A4', '#B4E1FF', '#F06C9B',
                  '#D3C4D1', '#81F4E1', '#C2AFF0', '#C57B57']

labels_members = top10_members['Name'].values
values_members = top10_members['Members'].values

plt.barh(labels_members, values_members, color=colors_members, edgecolor='black')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='x', alpha=0.7)
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
plt.title("Top 10 Anime Community", fontsize=20, fontweight='bold')
plt.xlabel('Members Count', fontsize=15)
plt.ylabel('Anime Name', fontsize=15)
plt.tight_layout()
plt.show()

# Top 10 Anime berdasarkan Score
plt.figure(figsize=(20, 15))
top10_scores = df[['Name', 'Score']].sort_values(by='Score', ascending=False).head(10)

colors_scores = ['#87255B', '#56CBF9', '#F5D491', '#BEB7A4', '#B4E1FF', '#F06C9B',
                 '#D3C4D1', '#81F4E1', '#C2AFF0', '#C57B57']

labels_scores = top10_scores['Name'].values
values_scores = top10_scores['Score'].values

plt.barh(labels_scores, values_scores, color=colors_scores, edgecolor='black')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='x', alpha=0.7)
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
plt.title("Top 10 Anime Rating", fontsize=20, fontweight='bold')
plt.xlabel('Score', fontsize=15)
plt.ylabel('Anime Name', fontsize=15)
plt.tight_layout()
plt.show()

"""## Data Preparation"""

import re

def text_cleaning(text):
  text = re.sub(r"[^\w\s]", "", text)
  text = re.sub(r"https?://[^\s]+", "", text)
  return text

df['Name'] = df['Name'].apply(text_cleaning)

"""Remove alphanumeric punctuation and Remove links (URLs)"""

df

df.duplicated().sum()

"""No duplicate data"""

df.isnull().sum()

"""There are *Missing values* in the `sypnopsis and Ranked` columns"""

df = df.dropna()

"""Removing *Missing value* data"""

df.isnull().sum()

print(f'This dataset has {df.shape[0]} rows and {df.shape[1]} columns.')

"""The original number of datasets was `14952` and by removing the number of *missing values* the dataset is now `13229`."""

df.describe().T

"""The `describe()` function provides statistical information on each column, including:

- `Count` is the number of samples in the data.
- `Mean` is the average value.
- `Std` is the standard deviation.
- `Min` is the minimum value of each column.
- `25%` is the first quartile. Quartiles are values that mark the boundaries of intervals in four equal parts of the distribution.
- `50%` is the second quartile, also called the median. - 75%` is the third quartile.
- `Max` is the maximum value.

## Modelling
"""

df

data = df.drop(columns=['anime_id',
                        'Episodes',
                        'English name',
                        'Japanese name',
                        'sypnopsis',
                        'Episodes',
                        'Aired',
                        'Premiered',
                        'Producers',
                        'Licensors',
                        'Source',
                        'Duration',
                        'Rating',
                        'Ranked',
                        'Popularity',
                        'Members',
                        'Favorites',
                        'Watching',
                        'Completed',
                        'On-Hold',
                        'Dropped'])

"""Remove columns that are not needed in this model."""

data

"""### Model Content Based Filtering (dengan Filter Genres)"""

tfid = TfidfVectorizer()
tfid.fit(data['Genres'])

tfid.get_feature_names_out()

tfidf_matrix = tfid.fit_transform(data['Genres'])


tfidf_matrix.shape

tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfid.get_feature_names_out(),
    index=data.Genres
).sample(22, axis=1).sample(10, axis=0)

"""The above `tf-idf matrix` output shows the relationship between the anime name against the selected category. This matrix shows how much correlation between Anime and the selected category."""

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

cosine_sim_df = pd.DataFrame(cosine_sim, index=data['Name'], columns=data['Name'])
print('Shape:', cosine_sim_df.shape)

# View the similarity matrix for each restaurant
cosine_sim_df.sample(5, axis=1).sample(5, axis=0)

def anime_recommendations(anime_name, similarity_data=cosine_sim_df, items=data[['Name','Genres']], k=5):


    index = similarity_data.loc[:,anime_name].to_numpy().argpartition(
        range(-1, -k, -1))


    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(anime_name, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

data[data.Name.eq('One Piece')]

anime_recommendations('One Piece')

"""The system has successfully recommended the top 5 percent of anime similar to *One Piece*, which are several movies and series from *One Piece* itself. So, if the user likes *One Piece*, then the system can recommend other *One Piece* series or movies.

### Model K-Nearest Neighbor
"""

animedf_name = pd.DataFrame({'Name':data['Name']})
animedf_name.head()

data.set_index('Name',inplace=True)

data_n = data[['Score','Type','Studios']]

data_new = pd.get_dummies(data_n[['Type','Studios']])
data_new = pd.concat([data_n, data_new], axis=1)
data_new = data_new.drop(columns='Type')
data_new = data_new.drop(columns='Studios')
data_new.head()

model = NearestNeighbors(metric='euclidean')
model.fit(data_new)

def Recommended_model(anime_name:str, recommend_anime:int=5):
  print(f'If the user likes app:{anime_name[0]} \nHere are apps that might also be liked:')
  distances, neighbors = model.kneighbors(data_new.loc[anime_name],n_neighbors=recommend_anime)
  similar_anime = []
  for anime_name in animedf_name.loc[neighbors[0][:]].values:
    similar_anime.append(anime_name[0])
  similar_distance = []
  for distance in distances[0]:
    similar_distance.append(f"{round(100-distance, 2)}%")
  return pd.DataFrame(data = {"Anime Name" : similar_anime, "Similiarity Score" : similar_distance})

Recommended_model(animedf_name.loc[21])

"""## Evaluation

### Calinski-Harabasz score

_**The Calinski-Harabasz score**_ is an evaluation metric for clustering algorithms that measures how well the clustering separates the data into compact, discrete groups. Defined as the ratio between the scatter within clusters and the scatter between clusters, the higher the CH value, the better the clustering performance, without requiring information about the ground truth labels.

**The Calinski-Harabasz Score** (CH) formula is:
$$CH = \frac{B}{W} \times \frac{N - k}{k - 1}$$

Where:
- \( B \) is the between-cluster scatter.
- \( W \) is the within-cluster scatter.
- \( N \) is the total amount of data.
- \( k \) is the number of clusters.
"""

ch_score = calinski_harabasz_score(data_new, animedf_name)

ch_score

"""The evaluation results show that the clusters in this model are still not well separated, which is reflected in the relatively low Calinski-Harabasz (CH) score of `3.1613291729405617`. This indicates the potential for inappropriate recommendations in some applications, which may not fully match user preferences. Therefore, further review or adjustments to the model are needed to improve cluster separation and recommendation accuracy.

### Davies Bouldin Score

_**Davies Bouldin Score (DB)**_ is a clustering performance evaluation metric that measures the average similarity of each cluster to its most similar cluster by comparing the distance within the cluster to the distance between clusters. With a minimum score of zero, the lower the DB value, the better the clustering performance, indicating clusters that are closer to each other and less dispersed. Different from most metrics, DB does not require a priori knowledge of ground truth labels, similar to Silhouette Score, but has a simpler formulation, providing an efficient way to evaluate clustering without requiring additional knowledge of the data structure.

The Davies-Bouldin Score (DB) formula is:


$$ DB = \frac{1}{k}\sum_{i=1}^{k}\max_{j\neq i}\left( \frac{R_i + R_j}{d(c_i, c_j)}\right) $$


Where:
- \( k \) is the number of clusters.
- \( R_i \) is the radius in the i-th cluster.
- \( d(c_i, c_j) \) is the distance between the i-th cluster center (\( c_i \)) and the j-th cluster center (\( c_j \)).

Davies-Bouldin is defined as the average of the R values, where each R value is the ratio between the sum of the radius within the cluster (in terms of distance, e.g. Euclidean distance) and the distance between the cluster centers, and the other centers. This ratio is used to evaluate the similarity of each cluster with other clusters.
"""

db_score = davies_bouldin_score(data_new, animedf_name)

db_score

"""The Davies-Bouldin (DB) evaluation results show that the model has a relatively small score, with a DB value of `0.7864266764751376` This indicates that the model already has a fairly good cluster separation. As a result, the anime recommendations are of good quality, considering that the cluster grouping in the model is already quite effective in separating the data. This is proven by the results of the application recommendations that are already quite good."""